# üõ†Ô∏è Data Pipeline Automation

Este projeto √© um estudo para automa√ß√£o do processamento de dados, abrangendo as etapas de extra√ß√£o, transforma√ß√£o e carregamento (ETL) utilizando PostgreSQL, Apache Airflow, Kafka, PySpark e Cassandra. O fluxo de dados √© projetado para extrair dados de um banco de dados PostgreSQL, process√°-los e carreg√°-los em um banco de dados Cassandra.

<img src="https://github.com/user-attachments/assets/e2b36c11-87bf-4b8a-a0c1-9e71ccf061f4" width="500" height="200"  alt="Image description">

## üéØ Estrutura do Projeto

1. **Extra√ß√£o de Dados**:
  - Os dados s√£o extra√≠dos do PostgreSQL utilizando o Apache Airflow.
  - Esses dados s√£o salvos localmente como arquivos CSV, organizados por data.
2. **Kafka Connect**: 
  - Um conector Kafka l√™ os arquivos CSV extra√≠dos e os publica em um t√≥pico Kafka (csv-topic).
  - O Kafka Connect est√° configurado para utilizar plugins como SpoolDir para ler arquivos CSV e DataStax Cassandra para carregar dados processados no Cassandra.
3. **PySpark**: 
  - PySpark consome os dados do t√≥pico Kafka, processa-os, e os prepara para o armazenamento.
  - O processamento pode incluir limpeza, transforma√ß√£o e agrega√ß√£o de dados conforme a necessidade.
4. **Cassandra**: 
  - Ap√≥s o processamento, os dados s√£o enviados ao Cassandra, onde s√£o armazenados e disponibilizados para consulta.
  - O Cassandra √© configurado para armazenar grandes volumes de dados, aproveitando a escalabilidade horizontal.

## üë∑ Servi√ßos Configurados no Docker Compose

O projeto utiliza uma configura√ß√£o Docker Compose para orquestrar os seguintes servi√ßos:

- **PostgreSQL (db1):** Banco de dados usado pelo Apache Airflow para armazenar metadados e tamb√©m pelos fluxos de trabalho para a extra√ß√£o de dados.

- **Apache Airflow (airflow-webserver, airflow-scheduler, airflow-init):**
  - O `airflow-webserver` fornece a interface de usu√°rio.
  - O `airflow-scheduler` gerencia a execu√ß√£o das DAGs.
  - O `airflow-init` realiza a configura√ß√£o inicial do Airflow.

- **Zookeeper (zookeeper) e Kafka Broker (broker):**
  - `Zookeeper` gerencia o cluster Kafka.
  - `Kafka Broker` gerencia o sistema de mensageria, permitindo a transmiss√£o de dados entre produtores e consumidores.

- **Kafka Connect (kafka-connect):** Configurado para utilizar o conector SpoolDir para leitura dos arquivos CSV e o conector DataStax Cassandra para carregar dados no Cassandra.

- **Schema Registry (schema-registry):** Gerencia os esquemas de dados transmitidos pelos t√≥picos Kafka.

- **Kafka Control Center (control-center):** Interface para monitoramento e gerenciamento do cluster Kafka.

- **Spark Master (spark-master) e Spark Worker (spark-worker):** Configurados para processamento distribu√≠do de dados utilizando PySpark.

- **Cassandra (cassandra_db):** Banco de dados NoSQL usado para armazenar os dados processados.

- **PostgreSQL (postgres):** Banco de dados usado pelo Airflow.

## ü§ñ Configura√ß√£o e Execu√ß√£o
1. **Arquivos**
   Insira o arquivo config_data.csv dentro da pasta data/current_data

2. **Inicie os containers**:
   ```bash
   docker compose up -d 

3. **Execute o script PySpark**
Conecte-se ao cont√™iner do Spark e execute o script PySpark para processar os dados:

```bash
docker exec -it data-pipeline-automation-spark-master-1 bash
cd /opt/bitnami/spark/pyspark
pip install py4j
pip install cassandra-driver
python main.py
```

4. **Extraia os dados do Airflow**
Acesse localhost:8080 
Rode a dag

5. **Verifique o resultado final no Cassandara**

Conecte-se ao cont√™iner do Cassandra e verifique os dados processados:
```bash
docker exec -it cassandra bash  
cqlsh
USE amazon;
## Para contar o n√∫mero de linhas na tabela sales, execute:
SELECT COUNT(*) FROM sales;
```

## üöÄ Conclus√£o

Este projeto demonstra a automa√ß√£o de um pipeline de dados completo, desde a extra√ß√£o at√© o armazenamento em um banco de dados NoSQL, utilizando ferramentas amplamente adotadas na ind√∫stria. A configura√ß√£o Docker Compose simplifica a orquestra√ß√£o dos servi√ßos, enquanto Apache Airflow, Kafka, PySpark, e Cassandra trabalham juntos para garantir um fluxo de dados eficiente e escal√°vel.

### Pr√≥ximos Passos

- **Monitoramento e Alertas**: Implementar ferramentas de monitoramento e alertas para garantir a robustez e a efici√™ncia do pipeline.
- **Escalabilidade**: Explorar a escalabilidade horizontal do Cassandra e do Spark para lidar com volumes de dados ainda maiores.
- **Seguran√ßa**: Adicionar medidas de seguran√ßa, como autentica√ß√£o e criptografia, para proteger os dados sens√≠veis durante o processo.
- **Documenta√ß√£o**: Continuar aprimorando a documenta√ß√£o do projeto, incluindo detalhes sobre a configura√ß√£o dos conectores Kafka e melhores pr√°ticas para o uso do PySpark.

Esse projeto serve como um estudo para um ponto de partida para a cria√ß√£o de pipelines de dados automatizados em ambientes de produ√ß√£o, permitindo o processamento de grandes volumes de dados de maneira eficiente e escal√°vel.

## üî® Linguagens, Tecnologias e Bibliotecas Utilizadas

<div style="display: flex; flex-direction: row;">
  <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/1200px-Python-logo-notext.svg.png" alt="Descri√ß√£o da Imagem" width="40">
  <img src="https://github.com/user-attachments/assets/e4183d59-0f47-4d7e-a773-0943ff36fb1f" alt="Descri√ß√£o da Imagem" width="80">
  <img src="https://static-00.iconduck.com/assets.00/airflow-icon-2048x2048-ptyvisqh.png" alt="Descri√ß√£o da Imagem" width="40">
  <img src="https://blog.geekhunter.com.br/wp-content/uploads/2020/09/apache-kafka.png" alt="Descri√ß√£o da Imagem" width="40">
  <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/1280px-Apache_Spark_logo.svg.png" alt="Descri√ß√£o da Imagem" width="80">
  <img src="https://download.logo.wine/logo/Apache_Cassandra/Apache_Cassandra-Logo.wine.png" alt="Descri√ß√£o da Imagem" width="80">
  <img src="https://cdn.icon-icons.com/icons2/2415/PNG/512/postgresql_plain_wordmark_logo_icon_146390.png" alt="Descri√ß√£o da Imagem" width="40">
  <img src="https://logosmarcas.net/wp-content/uploads/2021/03/Docker-Logo.png" alt="Descri√ß√£o da Imagem" width="80"> 
</div>
